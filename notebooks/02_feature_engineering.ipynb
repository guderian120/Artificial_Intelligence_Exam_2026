{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMI607: AI and Machine Learning Take-Home Exam\n",
    "## Section C: Feature Engineering & Representation (Question 3)\n",
    "\n",
    "**Objective**: Apply and justify feature engineering techniques with domain knowledge.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 37 column 24 (char 756)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m df = pd.read_pickle(\u001b[33m'\u001b[39m\u001b[33mdata/preprocessed_data.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mdata/metadata.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     metadata = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m target_col = metadata[\u001b[33m'\u001b[39m\u001b[33mtarget_column\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded preprocessed data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m     end = _w(s, end).end()\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    361\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 37 column 24 (char 756)"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "import os\n",
    "\n",
    "# Check if preprocessed data exists, otherwise load from Kaggle\n",
    "if os.path.exists('data/preprocessed_data.pkl'):\n",
    "    df = pd.read_pickle('data/preprocessed_data.pkl')\n",
    "    with open('data/metadata.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    target_col = metadata['target_column']\n",
    "    print(f\"Loaded preprocessed data: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "else:\n",
    "    import kagglehub\n",
    "    path = kagglehub.dataset_download(\"ziya07/iiot-edge-computing-dataset\")\n",
    "    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "    df = pd.read_csv(os.path.join(path, csv_files[0]))\n",
    "    # Identify target column\n",
    "    potential_targets = [col for col in df.columns if any(t in col.lower() for t in ['target', 'label', 'class', 'anomaly', 'attack'])]\n",
    "    target_col = potential_targets[0] if potential_targets else df.columns[-1]\n",
    "    print(f\"Loaded data from Kaggle: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "\n",
    "print(f\"Target column: {target_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Analysis and Domain Knowledge\n",
    "\n",
    "### Domain Context: IIoT Edge Computing & Infrastructure Monitoring\n",
    "\n",
    "The dataset contains sensor readings from Industrial IoT edge computing devices. Key domain insights:\n",
    "\n",
    "1. **Sensor readings** often have drift and calibration issues\n",
    "2. **Temporal patterns** are crucial for anomaly detection\n",
    "3. **Cross-sensor correlations** reveal system state\n",
    "4. **Statistical features** capture distribution changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\nFeature Summary:\")\n",
    "print(f\"  â€¢ Numeric features: {len(numeric_cols)}\")\n",
    "print(f\"  â€¢ Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"  â€¢ Target classes: {y.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Feature Representation Technique 1: Statistical Feature Engineering\n",
    "\n",
    "**Justification**: IIoT sensor data exhibits statistical patterns that raw values don't capture. Creating statistical features helps models understand data distribution characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Representation 1: Statistical Features\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE REPRESENTATION 1: STATISTICAL ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_numeric = X[numeric_cols].copy()\n",
    "\n",
    "# Handle any remaining missing values\n",
    "X_numeric = X_numeric.fillna(X_numeric.median())\n",
    "\n",
    "# Create statistical features\n",
    "stat_features = pd.DataFrame(index=X_numeric.index)\n",
    "\n",
    "# Row-wise statistics (for each sample across all features)\n",
    "stat_features['row_mean'] = X_numeric.mean(axis=1)\n",
    "stat_features['row_std'] = X_numeric.std(axis=1)\n",
    "stat_features['row_min'] = X_numeric.min(axis=1)\n",
    "stat_features['row_max'] = X_numeric.max(axis=1)\n",
    "stat_features['row_range'] = stat_features['row_max'] - stat_features['row_min']\n",
    "stat_features['row_skew'] = X_numeric.skew(axis=1)\n",
    "stat_features['row_kurtosis'] = X_numeric.kurtosis(axis=1)\n",
    "\n",
    "# Percentile-based features\n",
    "stat_features['row_q25'] = X_numeric.quantile(0.25, axis=1)\n",
    "stat_features['row_q75'] = X_numeric.quantile(0.75, axis=1)\n",
    "stat_features['row_iqr'] = stat_features['row_q75'] - stat_features['row_q25']\n",
    "\n",
    "# Combine with original features\n",
    "X_stat = pd.concat([X_numeric, stat_features], axis=1)\n",
    "\n",
    "print(f\"\\nðŸ“Š Statistical Features Created:\")\n",
    "print(f\"   â€¢ Original features: {len(numeric_cols)}\")\n",
    "print(f\"   â€¢ New statistical features: {len(stat_features.columns)}\")\n",
    "print(f\"   â€¢ Total features: {X_stat.shape[1]}\")\n",
    "print(f\"\\n   Features added: {list(stat_features.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate impact of statistical features\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Baseline model (original features only)\n",
    "scaler = StandardScaler()\n",
    "X_orig_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "rf_baseline = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "scores_baseline = cross_val_score(rf_baseline, X_orig_scaled, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Model with statistical features\n",
    "X_stat_scaled = scaler.fit_transform(X_stat)\n",
    "rf_stat = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "scores_stat = cross_val_score(rf_stat, X_stat_scaled, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nðŸ“ˆ PERFORMANCE COMPARISON:\")\n",
    "print(f\"   Baseline (original features): {scores_baseline.mean():.4f} (+/- {scores_baseline.std()*2:.4f})\")\n",
    "print(f\"   With statistical features:    {scores_stat.mean():.4f} (+/- {scores_stat.std()*2:.4f})\")\n",
    "print(f\"   Improvement: {(scores_stat.mean() - scores_baseline.mean())*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Representation Technique 2: Dimensionality Reduction with PCA\n",
    "\n",
    "**Justification**: High-dimensional IIoT data often contains redundant information. PCA extracts principal components that capture maximum variance while reducing dimensionality for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Representation 2: PCA-based Representation\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE REPRESENTATION 2: PCA DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scale data for PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Apply PCA\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "n_components_90 = np.argmax(cumsum >= 0.90) + 1\n",
    "\n",
    "print(f\"\\nðŸ“Š PCA ANALYSIS:\")\n",
    "print(f\"   â€¢ Original dimensions: {X_scaled.shape[1]}\")\n",
    "print(f\"   â€¢ Components for 90% variance: {n_components_90}\")\n",
    "print(f\"   â€¢ Components for 95% variance: {n_components_95}\")\n",
    "print(f\"   â€¢ Dimensionality reduction: {(1 - n_components_95/X_scaled.shape[1])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Explained variance by component\n",
    "ax1 = axes[0]\n",
    "n_show = min(20, len(pca_full.explained_variance_ratio_))\n",
    "ax1.bar(range(1, n_show+1), pca_full.explained_variance_ratio_[:n_show], alpha=0.7, label='Individual')\n",
    "ax1.plot(range(1, n_show+1), cumsum[:n_show], 'r-o', linewidth=2, label='Cumulative')\n",
    "ax1.axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "ax1.set_xlabel('Principal Component', fontsize=12)\n",
    "ax1.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax1.set_title('PCA Explained Variance Analysis', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: 2D PCA projection\n",
    "ax2 = axes[1]\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "# Sample for visualization if dataset is large\n",
    "sample_size = min(5000, len(X_pca_2d))\n",
    "indices = np.random.choice(len(X_pca_2d), sample_size, replace=False)\n",
    "\n",
    "scatter = ax2.scatter(X_pca_2d[indices, 0], X_pca_2d[indices, 1], \n",
    "                      c=y.iloc[indices].astype('category').cat.codes, \n",
    "                      cmap='viridis', alpha=0.5, s=10)\n",
    "ax2.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% var)', fontsize=12)\n",
    "ax2.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% var)', fontsize=12)\n",
    "ax2.set_title('2D PCA Projection of Data', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax2, label='Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/02_pca_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nâœ… Figure saved: images/02_pca_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PCA-transformed features\n",
    "pca_opt = PCA(n_components=n_components_95)\n",
    "X_pca = pca_opt.fit_transform(X_scaled)\n",
    "\n",
    "rf_pca = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "scores_pca = cross_val_score(rf_pca, X_pca, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nðŸ“ˆ PCA PERFORMANCE COMPARISON:\")\n",
    "print(f\"   Baseline (original {X_scaled.shape[1]} features): {scores_baseline.mean():.4f} (+/- {scores_baseline.std()*2:.4f})\")\n",
    "print(f\"   PCA ({n_components_95} components):              {scores_pca.mean():.4f} (+/- {scores_pca.std()*2:.4f})\")\n",
    "print(f\"   Accuracy change: {(scores_pca.mean() - scores_baseline.mean())*100:+.2f}%\")\n",
    "print(f\"   Feature reduction: {(1 - n_components_95/X_scaled.shape[1])*100:.1f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# LDA for dimensionality reduction (max n_components = n_classes - 1)\n",
    "lda = LinearDiscriminantAnalysis(n_components=min(len(np.unique(y))-1, 50))\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "rf_lda = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "scores_lda = cross_val_score(rf_lda, X_lda, y, cv=5, scoring='accuracy')\n",
    "print(f\"LDA: {scores_lda.mean():.4f} (+/- {scores_lda.std()*2:.4f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Selection: Information-Theoretic Approach\n",
    "\n",
    "**Justification**: Select the most informative features using mutual information, which captures non-linear dependencies that correlation-based methods miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using Mutual Information\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE SELECTION: MUTUAL INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate mutual information scores\n",
    "mi_scores = mutual_info_classif(X_scaled, y, random_state=42)\n",
    "mi_df = pd.DataFrame({'Feature': numeric_cols, 'MI_Score': mi_scores})\n",
    "mi_df = mi_df.sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ“Š TOP 15 FEATURES BY MUTUAL INFORMATION:\")\n",
    "print(mi_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize feature importance\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# # Plot 1: Mutual Information scores\n",
    "# ax1 = axes[0]\n",
    "# top_n = 15\n",
    "# top_features = mi_df.head(top_n)\n",
    "# colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, top_n))\n",
    "# bars = ax1.barh(range(top_n), top_features['MI_Score'].values[::-1], color=colors[::-1])\n",
    "# ax1.set_yticks(range(top_n))\n",
    "# ax1.set_yticklabels(top_features['Feature'].values[::-1])\n",
    "# ax1.set_xlabel('Mutual Information Score', fontsize=12)\n",
    "# ax1.set_title('Top 15 Features by Mutual Information', fontsize=14, fontweight='bold')\n",
    "# ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# # Plot 2: Feature selection comparison\n",
    "# ax2 = axes[1]\n",
    "# k_values = [5, 10, 15, 20, 25, 30]\n",
    "# k_scores = []\n",
    "# for k in k_values:\n",
    "#     top_k_features = mi_df.head(k)['Feature'].tolist()\n",
    "#     X_selected = X_numeric[top_k_features]\n",
    "#     X_sel_scaled = StandardScaler().fit_transform(X_selected)\n",
    "#     rf_sel = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "#     scores = cross_val_score(rf_sel, X_sel_scaled, y, cv=3, scoring='accuracy')\n",
    "#     k_scores.append(scores.mean())\n",
    "\n",
    "# ax2.plot(k_values, k_scores, 'bo-', linewidth=2, markersize=8)\n",
    "# ax2.axhline(y=scores_baseline.mean(), color='r', linestyle='--', label='All features baseline')\n",
    "# ax2.set_xlabel('Number of Selected Features', fontsize=12)\n",
    "# ax2.set_ylabel('Cross-Validation Accuracy', fontsize=12)\n",
    "# ax2.set_title('Accuracy vs Number of Features', fontsize=14, fontweight='bold')\n",
    "# ax2.legend()\n",
    "# ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('images/02_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# print(\"\\nâœ… Figure saved: images/02_feature_importance.png\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 1: Mutual Information scores\n",
    "# ============================================================================\n",
    "ax1 = axes[0]\n",
    "top_n = 5\n",
    "top_features = mi_df.head(top_n)\n",
    "\n",
    "# Create colors array matching the number of features\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(top_features)))\n",
    "\n",
    "# Create horizontal bar chart\n",
    "bars = ax1.barh(range(len(top_features)), \n",
    "                top_features['MI_Score'].values[::-1], \n",
    "                color=colors[::-1])\n",
    "\n",
    "ax1.set_yticks(range(len(top_features)))\n",
    "ax1.set_yticklabels(top_features['Feature'].values[::-1], fontsize=10)\n",
    "ax1.set_xlabel('Mutual Information Score', fontsize=12)\n",
    "ax1.set_title('Top 5 Features by Mutual Information', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, value) in enumerate(zip(bars, top_features['MI_Score'].values[::-1])):\n",
    "    ax1.text(value, bar.get_y() + bar.get_height()/2, \n",
    "             f'{value:.3f}', \n",
    "             ha='left', va='center', fontsize=9, \n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 2: Feature selection comparison\n",
    "# ============================================================================\n",
    "ax2 = axes[1]\n",
    "k_values = [5, 10, 15, 20, 25, 30, 40, 50]\n",
    "k_scores = []\n",
    "\n",
    "print(\"\\nðŸ” Testing different numbers of features...\")\n",
    "for k in k_values:\n",
    "    # Ensure k doesn't exceed available features\n",
    "    k_actual = min(k, len(mi_df))\n",
    "    top_k_features = mi_df.head(k_actual)['Feature'].tolist()\n",
    "    \n",
    "    X_selected = X_numeric[top_k_features]\n",
    "    X_sel_scaled = StandardScaler().fit_transform(X_selected)\n",
    "    \n",
    "    rf_sel = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "    scores = cross_val_score(rf_sel, X_sel_scaled, y, cv=3, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "    print(f\"   k={k_actual:2d}: Accuracy = {scores.mean():.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "ax2.plot(k_values[:len(k_scores)], k_scores, 'bo-', linewidth=2, markersize=8, label='Selected features')\n",
    "ax2.axhline(y=scores_baseline.mean(), color='r', linestyle='--', linewidth=2, label=f'Baseline ({scores_baseline.mean():.4f})')\n",
    "\n",
    "# Mark the best point\n",
    "best_idx = np.argmax(k_scores)\n",
    "best_k = k_values[best_idx]\n",
    "best_score = k_scores[best_idx]\n",
    "ax2.plot(best_k, best_score, 'g*', markersize=20, label=f'Best: k={best_k} ({best_score:.4f})')\n",
    "\n",
    "ax2.set_xlabel('Number of Selected Features', fontsize=12)\n",
    "ax2.set_ylabel('Cross-Validation Accuracy', fontsize=12)\n",
    "ax2.set_title('Accuracy vs Number of Features', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentage improvement annotations\n",
    "for i, (k, score) in enumerate(zip(k_values[:len(k_scores)], k_scores)):\n",
    "    improvement = (score - scores_baseline.mean()) * 100\n",
    "    if abs(improvement) > 0.5:  # Only show significant changes\n",
    "        ax2.annotate(f'{improvement:+.1f}%', \n",
    "                    xy=(k, score), \n",
    "                    xytext=(5, 5), \n",
    "                    textcoords='offset points',\n",
    "                    fontsize=8, \n",
    "                    alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/02_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Figure saved: images/02_feature_importance.png\")\n",
    "print(f\"\\nðŸ† Optimal number of features: {best_k} (Accuracy: {best_score:.4f})\")\n",
    "print(f\"   Improvement over baseline: {(best_score - scores_baseline.mean())*100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_data = {\n",
    "    'Representation': [\n",
    "        'Original Features',\n",
    "        'Statistical Features',\n",
    "        f'PCA ({n_components_95} components)',\n",
    "        f'Top-{top_n} by MI'\n",
    "    ],\n",
    "    'Num Features': [\n",
    "        len(numeric_cols),\n",
    "        X_stat.shape[1],\n",
    "        n_components_95,\n",
    "        top_n\n",
    "    ],\n",
    "    'CV Accuracy': [\n",
    "        f'{scores_baseline.mean():.4f}',\n",
    "        f'{scores_stat.mean():.4f}',\n",
    "        f'{scores_pca.mean():.4f}',\n",
    "        f'{k_scores[k_values.index(15)]:.4f}' if 15 in k_values else 'N/A'\n",
    "    ],\n",
    "    'Std Dev': [\n",
    "        f'{scores_baseline.std():.4f}',\n",
    "        f'{scores_stat.std():.4f}',\n",
    "        f'{scores_pca.std():.4f}',\n",
    "        'N/A'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Engineered features saved to 'data/engineered_features.pkl'\n",
      "âœ… Target variable saved to 'data/target.pkl'\n",
      "âœ… Metadata updated with feature engineering info\n"
     ]
    }
   ],
   "source": [
    "# Save engineered features for next notebook\n",
    "best_features = mi_df.head(20)['Feature'].tolist()\n",
    "X_engineered = X_stat[best_features + list(stat_features.columns)]\n",
    "\n",
    "# Save to file\n",
    "X_engineered.to_pickle('data/engineered_features.pkl')\n",
    "y.to_pickle('data/target.pkl')\n",
    "\n",
    "# Update metadata\n",
    "with open('data/metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "metadata['selected_features'] = best_features\n",
    "metadata['statistical_features'] = list(stat_features.columns)\n",
    "metadata['pca_components_95'] = str(n_components_95)\n",
    "\n",
    "with open('data/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Engineered features saved to 'data/engineered_features.pkl'\")\n",
    "print(\"âœ… Target variable saved to 'data/target.pkl'\")\n",
    "print(\"âœ… Metadata updated with feature engineering info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Feature Engineering Techniques Applied:\n",
    "\n",
    "| Technique | Description | Domain Justification | Impact |\n",
    "|-----------|-------------|---------------------|--------|\n",
    "| Statistical Features | Row-wise mean, std, skew, kurtosis, IQR | Captures distribution changes in sensor readings | Improved accuracy |\n",
    "| PCA Reduction | Extract principal components for 95% variance | Removes redundant information, speeds up training | Minimal accuracy loss, faster training |\n",
    "| Mutual Information Selection | Select top features by information content | Identifies most predictive features for anomaly detection | Focused model with key features |\n",
    "\n",
    "### Key Insights:\n",
    "1. Statistical features enhance model understanding of data distribution\n",
    "2. PCA achieves significant dimensionality reduction with minimal information loss\n",
    "3. Feature selection identifies the most informative features for the classification task\n",
    "\n",
    "**Next >>**: Machine Learning Model Design (Notebook 03)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
