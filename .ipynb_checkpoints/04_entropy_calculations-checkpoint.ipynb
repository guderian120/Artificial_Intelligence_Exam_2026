{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# WMI607: AI and Machine Learning Take-Home Exam\n",
                "## Section G: Dataset Generation & Entropy Computation (Questions 7-12)\n",
                "\n",
                "---\n",
                "\n",
                "**Note**: This section requires your **Student ID** as the random seed to generate a unique dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from collections import Counter\n",
                "import math\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Question 7: Dataset Generation (4 Marks)\n",
                "\n",
                "**Task**: Generate a unique dataset of 24 instances with 3 categorical features and 1 binary class label."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# =============================================================================\n",
                "# ‚ö†Ô∏è IMPORTANT: Replace with your actual Student ID\n",
                "# =============================================================================\n",
                "STUDENT_ID = 12345678  # <-- REPLACE THIS WITH YOUR STUDENT ID\n",
                "# =============================================================================\n",
                "\n",
                "print(f\"Student ID (Random Seed): {STUDENT_ID}\")\n",
                "\n",
                "# Set the random seed using student ID\n",
                "np.random.seed(STUDENT_ID)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define feature values (each feature has 2-3 possible values)\n",
                "# Feature justification based on infrastructure monitoring domain (related to MSc project)\n",
                "\n",
                "# Feature 1: Configuration State - relates to drift detection scenarios\n",
                "feature1_values = ['Compliant', 'Modified', 'Unknown']\n",
                "feature1_name = 'Config_State'\n",
                "\n",
                "# Feature 2: Resource Type - type of cloud resource being monitored\n",
                "feature2_values = ['Compute', 'Storage', 'Network']\n",
                "feature2_name = 'Resource_Type'\n",
                "\n",
                "# Feature 3: Change Frequency - how often the resource changes\n",
                "feature3_values = ['Low', 'High']\n",
                "feature3_name = 'Change_Freq'\n",
                "\n",
                "# Class label: Whether anomaly/drift was detected\n",
                "class_values = ['No', 'Yes']\n",
                "class_name = 'Anomaly_Detected'\n",
                "\n",
                "print(\"Feature Definitions:\")\n",
                "print(f\"  {feature1_name}: {feature1_values}\")\n",
                "print(f\"  {feature2_name}: {feature2_values}\")\n",
                "print(f\"  {feature3_name}: {feature3_values}\")\n",
                "print(f\"  {class_name}: {class_values}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate 24 random instances\n",
                "n_instances = 24\n",
                "\n",
                "data = {\n",
                "    feature1_name: np.random.choice(feature1_values, n_instances),\n",
                "    feature2_name: np.random.choice(feature2_values, n_instances),\n",
                "    feature3_name: np.random.choice(feature3_values, n_instances),\n",
                "    class_name: np.random.choice(class_values, n_instances)\n",
                "}\n",
                "\n",
                "# Create DataFrame\n",
                "df = pd.DataFrame(data)\n",
                "\n",
                "# Add instance numbers for reference\n",
                "df.insert(0, 'Instance', range(1, n_instances + 1))\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"GENERATED DATASET (24 Instances)\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"\\nRandom Seed (Student ID): {STUDENT_ID}\\n\")\n",
                "print(df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature choice justification\n",
                "print(f\"\"\"\n",
                "{'='*60}\n",
                "FEATURE CHOICE JUSTIFICATION\n",
                "{'='*60}\n",
                "\n",
                "The features are designed to align with my MSc project on Cloud Infrastructure Drift Detection:\n",
                "\n",
                "1. {feature1_name} ({feature1_values}):\n",
                "   - Represents the configuration state of a cloud resource\n",
                "   - 'Compliant' = matches expected IaC definition\n",
                "   - 'Modified' = has been changed from expected state\n",
                "   - 'Unknown' = state could not be determined\n",
                "\n",
                "2. {feature2_name} ({feature2_values}):\n",
                "   - Type of cloud infrastructure resource\n",
                "   - Different resources have different drift patterns\n",
                "   - Relevant to multi-resource drift detection systems\n",
                "\n",
                "3. {feature3_name} ({feature3_values}):\n",
                "   - Frequency of configuration changes\n",
                "   - High-frequency changes may indicate automated processes\n",
                "   - Low-frequency changes may warrant more scrutiny\n",
                "\n",
                "4. {class_name} (Binary: Yes/No):\n",
                "   - Whether an anomaly/drift was detected\n",
                "   - The target variable for classification\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset statistics\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"DATASET STATISTICS\")\n",
                "print(f\"{'='*60}\\n\")\n",
                "\n",
                "for col in [feature1_name, feature2_name, feature3_name, class_name]:\n",
                "    print(f\"{col} Distribution:\")\n",
                "    counts = df[col].value_counts()\n",
                "    for val, count in counts.items():\n",
                "        print(f\"  {val}: {count} ({count/n_instances*100:.1f}%)\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Question 8: Entropy of Class Label (2.5 Marks)\n",
                "\n",
                "**Task**: Compute the entropy of the class label with all intermediate steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Entropy calculation function\n",
                "def calculate_entropy(labels):\n",
                "    \"\"\"Calculate entropy of a label distribution.\"\"\"\n",
                "    total = len(labels)\n",
                "    if total == 0:\n",
                "        return 0\n",
                "    \n",
                "    counts = Counter(labels)\n",
                "    entropy = 0\n",
                "    \n",
                "    steps = []  # For showing work\n",
                "    \n",
                "    for label, count in counts.items():\n",
                "        if count > 0:\n",
                "            p = count / total\n",
                "            contribution = -p * math.log2(p)\n",
                "            entropy += contribution\n",
                "            steps.append({\n",
                "                'label': label,\n",
                "                'count': count,\n",
                "                'probability': p,\n",
                "                'log2_p': math.log2(p),\n",
                "                'contribution': contribution\n",
                "            })\n",
                "    \n",
                "    return entropy, steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate class label entropy\n",
                "class_labels = df[class_name].values\n",
                "class_counts = Counter(class_labels)\n",
                "\n",
                "print(f\"{'='*60}\")\n",
                "print(\"ENTROPY CALCULATION FOR CLASS LABEL\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "print(f\"\\nüìã CLASS DISTRIBUTION:\")\n",
                "total = len(class_labels)\n",
                "for label, count in class_counts.items():\n",
                "    print(f\"   {label}: {count} instances\")\n",
                "print(f\"   Total: {total} instances\")\n",
                "\n",
                "print(f\"\\nüìê ENTROPY FORMULA:\")\n",
                "print(f\"   H(S) = -Œ£ p(x) √ó log‚ÇÇ(p(x))\")\n",
                "print(f\"   where p(x) is the probability of class x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step-by-step entropy calculation\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"STEP-BY-STEP CALCULATION\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "entropy, steps = calculate_entropy(class_labels)\n",
                "\n",
                "for i, step in enumerate(steps, 1):\n",
                "    print(f\"\\nStep {i}: Class = '{step['label']}'\")\n",
                "    print(f\"   Count = {step['count']}\")\n",
                "    print(f\"   p({step['label']}) = {step['count']}/{total} = {step['probability']:.6f}\")\n",
                "    print(f\"   log‚ÇÇ({step['probability']:.6f}) = {step['log2_p']:.6f}\")\n",
                "    print(f\"   -p √ó log‚ÇÇ(p) = -{step['probability']:.6f} √ó {step['log2_p']:.6f}\")\n",
                "    print(f\"                = {step['contribution']:.6f}\")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"FINAL ENTROPY CALCULATION:\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"\\n   H(S) = \", end=\"\")\n",
                "print(\" + \".join([f\"{step['contribution']:.6f}\" for step in steps]))\n",
                "print(f\"\\n   H(S) = {entropy:.6f} bits\")\n",
                "print(f\"\\n   Rounded: H(S) ‚âà {entropy:.4f} bits\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interpretation of entropy\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"ENTROPY INTERPRETATION\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "print(f\"\"\"\n",
                "üìä WHAT THE ENTROPY VALUE MEANS:\n",
                "\n",
                "Calculated Entropy: H(S) = {entropy:.4f} bits\n",
                "\n",
                "Interpretation:\n",
                "\"\"\")\n",
                "\n",
                "if entropy == 0:\n",
                "    print(\"  ‚Ä¢ Entropy = 0: All instances belong to the same class\")\n",
                "    print(\"  ‚Ä¢ No uncertainty in classification\")\n",
                "    print(\"  ‚Ä¢ The dataset is perfectly pure\")\n",
                "elif entropy == 1:\n",
                "    print(\"  ‚Ä¢ Entropy = 1: Perfect balance (50% each class)\")\n",
                "    print(\"  ‚Ä¢ Maximum uncertainty for binary classification\")\n",
                "    print(\"  ‚Ä¢ Requires maximum information to determine class\")\n",
                "elif entropy < 0.5:\n",
                "    print(\"  ‚Ä¢ Low entropy: Classes are imbalanced\")\n",
                "    print(\"  ‚Ä¢ One class dominates, less uncertainty\")\n",
                "    print(\"  ‚Ä¢ Easier to predict the majority class\")\n",
                "elif entropy < 0.9:\n",
                "    print(\"  ‚Ä¢ Moderate entropy: Some class imbalance\")\n",
                "    print(\"  ‚Ä¢ Moderate uncertainty in classification\")\n",
                "    print(\"  ‚Ä¢ Classification requires some information\")\n",
                "else:\n",
                "    print(\"  ‚Ä¢ High entropy: Classes are fairly balanced\")\n",
                "    print(\"  ‚Ä¢ High uncertainty in classification\")\n",
                "    print(\"  ‚Ä¢ Requires more information to classify correctly\")\n",
                "\n",
                "# Show class balance\n",
                "yes_count = class_counts.get('Yes', 0)\n",
                "no_count = class_counts.get('No', 0)\n",
                "print(f\"\\nClass Balance: Yes={yes_count}/{total} ({yes_count/total*100:.1f}%), No={no_count}/{total} ({no_count/total*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Question 9: Conditional Entropy and Information Gain (6 Marks)\n",
                "\n",
                "**Task**: For each feature, compute conditional entropy, Information Gain, and rank features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_conditional_entropy(df, feature, target):\n",
                "    \"\"\"Calculate conditional entropy H(Target|Feature).\"\"\"\n",
                "    total = len(df)\n",
                "    conditional_entropy = 0\n",
                "    feature_values = df[feature].unique()\n",
                "    \n",
                "    details = []\n",
                "    \n",
                "    for value in feature_values:\n",
                "        subset = df[df[feature] == value]\n",
                "        subset_size = len(subset)\n",
                "        weight = subset_size / total\n",
                "        \n",
                "        if subset_size > 0:\n",
                "            subset_entropy, _ = calculate_entropy(subset[target].values)\n",
                "            contribution = weight * subset_entropy\n",
                "            conditional_entropy += contribution\n",
                "            \n",
                "            details.append({\n",
                "                'value': value,\n",
                "                'count': subset_size,\n",
                "                'weight': weight,\n",
                "                'entropy': subset_entropy,\n",
                "                'contribution': contribution,\n",
                "                'class_dist': dict(Counter(subset[target]))\n",
                "            })\n",
                "    \n",
                "    return conditional_entropy, details\n",
                "\n",
                "def calculate_information_gain(df, feature, target, total_entropy):\n",
                "    \"\"\"Calculate Information Gain IG(Target, Feature).\"\"\"\n",
                "    cond_entropy, details = calculate_conditional_entropy(df, feature, target)\n",
                "    ig = total_entropy - cond_entropy\n",
                "    return ig, cond_entropy, details"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate for each feature\n",
                "features = [feature1_name, feature2_name, feature3_name]\n",
                "total_entropy = entropy\n",
                "\n",
                "results = []\n",
                "\n",
                "print(f\"{'='*60}\")\n",
                "print(\"CONDITIONAL ENTROPY AND INFORMATION GAIN\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"\\nTotal Entropy H(S) = {total_entropy:.6f} bits\\n\")\n",
                "\n",
                "for feature in features:\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"FEATURE: {feature}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    ig, cond_entropy, details = calculate_information_gain(df, feature, class_name, total_entropy)\n",
                "    \n",
                "    print(f\"\\nüìã CONDITIONAL ENTROPY CALCULATION:\")\n",
                "    print(f\"   Formula: H(S|{feature}) = Œ£ (|Sv|/|S|) √ó H(Sv)\")\n",
                "    print(f\"\\n   Where Sv is subset where {feature} = v\\n\")\n",
                "    \n",
                "    for i, d in enumerate(details, 1):\n",
                "        print(f\"   Partition {i}: {feature} = '{d['value']}'\")\n",
                "        print(f\"      Count: {d['count']}/{total} = {d['weight']:.4f}\")\n",
                "        print(f\"      Class distribution: {d['class_dist']}\")\n",
                "        if d['entropy'] > 0:\n",
                "            print(f\"      H(Sv) = {d['entropy']:.6f}\")\n",
                "        else:\n",
                "            print(f\"      H(Sv) = 0 (pure subset)\")\n",
                "        print(f\"      Contribution: {d['weight']:.4f} √ó {d['entropy']:.6f} = {d['contribution']:.6f}\")\n",
                "        print()\n",
                "    \n",
                "    print(f\"   H(S|{feature}) = \", end=\"\")\n",
                "    print(\" + \".join([f\"{d['contribution']:.6f}\" for d in details]))\n",
                "    print(f\"   H(S|{feature}) = {cond_entropy:.6f} bits\")\n",
                "    \n",
                "    print(f\"\\nüìä INFORMATION GAIN:\")\n",
                "    print(f\"   IG({feature}) = H(S) - H(S|{feature})\")\n",
                "    print(f\"   IG({feature}) = {total_entropy:.6f} - {cond_entropy:.6f}\")\n",
                "    print(f\"   IG({feature}) = {ig:.6f} bits\")\n",
                "    \n",
                "    results.append({\n",
                "        'Feature': feature,\n",
                "        'Conditional_Entropy': cond_entropy,\n",
                "        'Information_Gain': ig\n",
                "    })"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Rank features by Information Gain\n",
                "results_df = pd.DataFrame(results)\n",
                "results_df = results_df.sort_values('Information_Gain', ascending=False).reset_index(drop=True)\n",
                "results_df['Rank'] = range(1, len(results_df) + 1)\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"FEATURE RANKING BY INFORMATION GAIN\")\n",
                "print(f\"{'='*60}\")\n",
                "print(\"\\n\" + results_df[['Rank', 'Feature', 'Conditional_Entropy', 'Information_Gain']].to_string(index=False))\n",
                "\n",
                "best_feature = results_df.iloc[0]['Feature']\n",
                "best_ig = results_df.iloc[0]['Information_Gain']\n",
                "print(f\"\\nüèÜ BEST FEATURE: {best_feature} (IG = {best_ig:.6f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Information Gain\n",
                "plt.figure(figsize=(10, 6))\n",
                "colors = ['#2ecc71' if f == best_feature else '#3498db' for f in results_df['Feature']]\n",
                "bars = plt.bar(results_df['Feature'], results_df['Information_Gain'], color=colors, edgecolor='black')\n",
                "plt.xlabel('Feature', fontsize=12)\n",
                "plt.ylabel('Information Gain (bits)', fontsize=12)\n",
                "plt.title('Information Gain Comparison by Feature', fontsize=14, fontweight='bold')\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add value labels\n",
                "for bar, ig in zip(bars, results_df['Information_Gain']):\n",
                "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
                "             f'{ig:.4f}', ha='center', va='bottom', fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('images/04_information_gain.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"\\n‚úÖ Figure saved: images/04_information_gain.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Question 10: Decision Tree Construction (1.5 Marks)\n",
                "\n",
                "**Task**: Select root node and construct first two levels of decision tree manually."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Root node selection\n",
                "print(f\"{'='*60}\")\n",
                "print(\"DECISION TREE CONSTRUCTION\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "print(f\"\\nüå≥ ROOT NODE SELECTION:\")\n",
                "print(f\"   Based on Information Gain analysis:\")\n",
                "for _, row in results_df.iterrows():\n",
                "    marker = \"‚Üí\" if row['Feature'] == best_feature else \" \"\n",
                "    print(f\"   {marker} {row['Feature']}: IG = {row['Information_Gain']:.6f}\")\n",
                "print(f\"\\n   ‚úÖ Root Node: {best_feature} (Highest IG = {best_ig:.6f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Construct first two levels\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"TREE STRUCTURE (First Two Levels)\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "root_values = df[best_feature].unique()\n",
                "remaining_features = [f for f in features if f != best_feature]\n",
                "\n",
                "tree_structure = {}\n",
                "\n",
                "print(f\"\\nLevel 0 (Root): {best_feature}\")\n",
                "print(f\"{'‚îÄ'*50}\")\n",
                "\n",
                "for value in root_values:\n",
                "    subset = df[df[best_feature] == value]\n",
                "    class_dist = dict(Counter(subset[class_name]))\n",
                "    subset_entropy, _ = calculate_entropy(subset[class_name].values)\n",
                "    \n",
                "    print(f\"\\n  Branch: {best_feature} = '{value}'\")\n",
                "    print(f\"  Samples: {len(subset)}, Class: {class_dist}\")\n",
                "    print(f\"  Entropy: {subset_entropy:.4f}\")\n",
                "    \n",
                "    if subset_entropy == 0:\n",
                "        # Pure node - leaf\n",
                "        majority_class = max(class_dist, key=class_dist.get)\n",
                "        print(f\"  ‚Üí LEAF NODE: {majority_class} (Pure)\")\n",
                "        tree_structure[value] = {'type': 'leaf', 'class': majority_class}\n",
                "    else:\n",
                "        # Need to split further - find best feature for this branch\n",
                "        best_sub_feature = None\n",
                "        best_sub_ig = -1\n",
                "        \n",
                "        for feature in remaining_features:\n",
                "            ig, _, _ = calculate_information_gain(subset, feature, class_name, subset_entropy)\n",
                "            if ig > best_sub_ig:\n",
                "                best_sub_ig = ig\n",
                "                best_sub_feature = feature\n",
                "        \n",
                "        print(f\"  ‚Üí SPLIT ON: {best_sub_feature} (IG = {best_sub_ig:.4f})\")\n",
                "        tree_structure[value] = {'type': 'split', 'feature': best_sub_feature, 'ig': best_sub_ig}\n",
                "        \n",
                "        # Show Level 2 branches\n",
                "        for sub_value in subset[best_sub_feature].unique():\n",
                "            sub_subset = subset[subset[best_sub_feature] == sub_value]\n",
                "            sub_class_dist = dict(Counter(sub_subset[class_name]))\n",
                "            majority = max(sub_class_dist, key=sub_class_dist.get)\n",
                "            print(f\"      ‚îî‚îÄ {best_sub_feature} = '{sub_value}': {sub_class_dist} ‚Üí {majority}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Draw decision tree diagram (text-based)\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"DECISION TREE DIAGRAM\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "print(f\"\"\"\n",
                "                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "                        ‚îÇ  [ROOT NODE]   ‚îÇ\n",
                "                        ‚îÇ  {best_feature:^12}  ‚îÇ\n",
                "                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "                                ‚îÇ\n",
                "\"\"\")\n",
                "\n",
                "# Generate branches dynamically\n",
                "for i, value in enumerate(root_values):\n",
                "    subset = df[df[best_feature] == value]\n",
                "    class_dist = dict(Counter(subset[class_name]))\n",
                "    subset_entropy, _ = calculate_entropy(subset[class_name].values)\n",
                "    \n",
                "    if subset_entropy == 0:\n",
                "        majority = max(class_dist, key=class_dist.get)\n",
                "        print(f\"    {value}: ‚Üí [{majority}] (Leaf)\")\n",
                "    else:\n",
                "        info = tree_structure[value]\n",
                "        print(f\"    {value}: ‚Üí Split on {info['feature']}\")\n",
                "\n",
                "print(\"\\n(See saved image for graphical representation)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use sklearn to visualize the tree for verification\n",
                "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "# Encode categorical variables\n",
                "le_dict = {}\n",
                "X_encoded = df[features].copy()\n",
                "for col in features:\n",
                "    le = LabelEncoder()\n",
                "    X_encoded[col] = le.fit_transform(X_encoded[col])\n",
                "    le_dict[col] = le\n",
                "\n",
                "y_encoded = LabelEncoder().fit_transform(df[class_name])\n",
                "\n",
                "# Train decision tree\n",
                "dt = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=42)\n",
                "dt.fit(X_encoded, y_encoded)\n",
                "\n",
                "# Plot tree\n",
                "plt.figure(figsize=(16, 10))\n",
                "plot_tree(dt, feature_names=features, class_names=class_values,\n",
                "          filled=True, rounded=True, fontsize=10)\n",
                "plt.title(f'Decision Tree (First 2 Levels) - Root: {best_feature}', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('images/04_decision_tree.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"\\n‚úÖ Figure saved: images/04_decision_tree.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Question 11: Modify Instance and Recompute (6 Marks)\n",
                "\n",
                "**Task**: Modify one data instance and analyze how entropy/IG changes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Original dataset stats\n",
                "print(f\"{'='*60}\")\n",
                "print(\"ORIGINAL DATASET (Before Modification)\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "original_class_dist = dict(Counter(df[class_name]))\n",
                "print(f\"\\nOriginal class distribution: {original_class_dist}\")\n",
                "print(f\"Original entropy: {entropy:.6f}\")\n",
                "print(f\"\\nOriginal Information Gains:\")\n",
                "for _, row in results_df.iterrows():\n",
                "    print(f\"   {row['Feature']}: {row['Information_Gain']:.6f}\")\n",
                "print(f\"\\nOriginal Root Node: {best_feature}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Modify one instance\n",
                "df_modified = df.copy()\n",
                "\n",
                "# Choose instance to modify (instance 1)\n",
                "modify_idx = 0\n",
                "original_instance = df_modified.iloc[modify_idx].copy()\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"INSTANCE MODIFICATION\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "print(f\"\\nOriginal Instance {modify_idx + 1}:\")\n",
                "print(f\"   {feature1_name}: {original_instance[feature1_name]}\")\n",
                "print(f\"   {feature2_name}: {original_instance[feature2_name]}\")\n",
                "print(f\"   {feature3_name}: {original_instance[feature3_name]}\")\n",
                "print(f\"   {class_name}: {original_instance[class_name]}\")\n",
                "\n",
                "# Flip the class label\n",
                "new_class = 'Yes' if original_instance[class_name] == 'No' else 'No'\n",
                "df_modified.loc[modify_idx, class_name] = new_class\n",
                "\n",
                "print(f\"\\nModified Instance {modify_idx + 1}:\")\n",
                "print(f\"   {class_name}: {original_instance[class_name]} ‚Üí {new_class}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Recompute entropy\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"RECOMPUTED ENTROPY\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "new_entropy, new_steps = calculate_entropy(df_modified[class_name].values)\n",
                "new_class_dist = dict(Counter(df_modified[class_name]))\n",
                "\n",
                "print(f\"\\nModified class distribution: {new_class_dist}\")\n",
                "\n",
                "print(f\"\\nEntropy Calculation:\")\n",
                "for step in new_steps:\n",
                "    print(f\"   p({step['label']}) = {step['probability']:.6f}\")\n",
                "    print(f\"   -p √ó log‚ÇÇ(p) = {step['contribution']:.6f}\")\n",
                "\n",
                "print(f\"\\nNew Entropy: H(S) = {new_entropy:.6f} bits\")\n",
                "print(f\"Entropy Change: {new_entropy - entropy:+.6f} bits\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Recompute Information Gains\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"RECOMPUTED INFORMATION GAINS\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "new_results = []\n",
                "for feature in features:\n",
                "    ig, cond_entropy, _ = calculate_information_gain(df_modified, feature, class_name, new_entropy)\n",
                "    new_results.append({\n",
                "        'Feature': feature,\n",
                "        'Original_IG': results_df[results_df['Feature'] == feature]['Information_Gain'].values[0],\n",
                "        'New_IG': ig,\n",
                "        'Change': ig - results_df[results_df['Feature'] == feature]['Information_Gain'].values[0]\n",
                "    })\n",
                "\n",
                "new_results_df = pd.DataFrame(new_results)\n",
                "new_results_df = new_results_df.sort_values('New_IG', ascending=False).reset_index(drop=True)\n",
                "\n",
                "print(\"\\n\" + new_results_df.to_string(index=False))\n",
                "\n",
                "new_best_feature = new_results_df.iloc[0]['Feature']\n",
                "new_best_ig = new_results_df.iloc[0]['New_IG']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analysis of root node change\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"ROOT NODE ANALYSIS\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "if new_best_feature == best_feature:\n",
                "    print(f\"\"\"\n",
                "üìä ROOT NODE: UNCHANGED\n",
                "\n",
                "   Original Root: {best_feature} (IG = {best_ig:.6f})\n",
                "   New Root:      {new_best_feature} (IG = {new_best_ig:.6f})\n",
                "\n",
                "   EXPLANATION:\n",
                "   The root node remains '{best_feature}' because:\n",
                "   1. The single instance modification did not significantly change the\n",
                "      relative information gain rankings of the features\n",
                "   2. {best_feature} still provides the best discrimination between classes\n",
                "   3. The class distribution change was not large enough to shift the \n",
                "      optimal splitting attribute\n",
                "\"\"\")\n",
                "else:\n",
                "    print(f\"\"\"\n",
                "üìä ROOT NODE: CHANGED!\n",
                "\n",
                "   Original Root: {best_feature} (IG = {best_ig:.6f})\n",
                "   New Root:      {new_best_feature} (IG = {new_best_ig:.6f})\n",
                "\n",
                "   EXPLANATION:\n",
                "   The root node changed from '{best_feature}' to '{new_best_feature}' because:\n",
                "   1. Modifying the instance altered the class distribution in subsets\n",
                "   2. This changed the conditional entropy calculations for each feature\n",
                "   3. {new_best_feature} now provides better information gain than {best_feature}\n",
                "   4. Small datasets are sensitive to individual instance changes\n",
                "\"\"\")\n",
                "\n",
                "print(f\"\\nSummary of Changes:\")\n",
                "print(f\"   Entropy: {entropy:.6f} ‚Üí {new_entropy:.6f} ({new_entropy-entropy:+.6f})\")\n",
                "for _, row in new_results_df.iterrows():\n",
                "    print(f\"   IG({row['Feature']}): {row['Original_IG']:.6f} ‚Üí {row['New_IG']:.6f} ({row['Change']:+.6f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Question 12: Information Gain Bias Discussion (5 Marks)\n",
                "\n",
                "**Task**: Discuss IG bias and compare with alternative splitting criteria (300-400 words)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate alternative metrics for comparison\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "# Train trees with different criteria\n",
                "dt_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=42)\n",
                "dt_gini = DecisionTreeClassifier(criterion='gini', max_depth=2, random_state=42)\n",
                "\n",
                "dt_entropy.fit(X_encoded, y_encoded)\n",
                "dt_gini.fit(X_encoded, y_encoded)\n",
                "\n",
                "print(f\"{'='*60}\")\n",
                "print(\"SPLITTING CRITERIA COMPARISON\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "print(f\"\\nEntropy-based (Information Gain):\")\n",
                "print(f\"   Root feature index: {dt_entropy.tree_.feature[0]} ({features[dt_entropy.tree_.feature[0]]})\")\n",
                "\n",
                "print(f\"\\nGini-based (Gini Impurity):\")\n",
                "print(f\"   Root feature index: {dt_gini.tree_.feature[0]} ({features[dt_gini.tree_.feature[0]]})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Essay on Information Gain Bias\n",
                "essay = f\"\"\"\n",
                "{'='*60}\n",
                "ESSAY: INFORMATION GAIN BIAS AND ALTERNATIVE CRITERIA\n",
                "{'='*60}\n",
                "\n",
                "WHY INFORMATION GAIN CAN BE BIASED:\n",
                "\n",
                "Information Gain (IG) is inherently biased toward features with more distinct \n",
                "values (higher cardinality). This bias occurs because:\n",
                "\n",
                "1. **Multi-value Advantage**: Features with more categories can partition the \n",
                "   data into smaller, potentially purer subsets. Even if this split is not \n",
                "   genuinely informative, the resulting subsets may have lower entropy simply \n",
                "   due to smaller sample sizes.\n",
                "\n",
                "2. **Extreme Case**: A unique identifier (like an ID field) would achieve \n",
                "   zero conditional entropy, making it appear as the \"best\" split despite \n",
                "   having no predictive value for unseen data.\n",
                "\n",
                "3. **Overfitting Risk**: This bias leads to trees that overfit on training \n",
                "   data by selecting features that won't generalize well.\n",
                "\n",
                "In our dataset:\n",
                "   - {feature1_name} has {len(feature1_values)} values\n",
                "   - {feature2_name} has {len(feature2_values)} values  \n",
                "   - {feature3_name} has {len(feature3_values)} values (fewer, potentially disadvantaged)\n",
                "\n",
                "ALTERNATIVE CRITERIA BEHAVIOR:\n",
                "\n",
                "**Gain Ratio (C4.5 Algorithm)**:\n",
                "Gain Ratio normalizes IG by the intrinsic information (split information) of \n",
                "the feature: GR(A) = IG(A) / SplitInfo(A). This penalizes features with many \n",
                "values, as their split information is higher. On our dataset, this would \n",
                "likely favor {feature3_name} relatively more, as it has only 2 values and \n",
                "thus lower split information.\n",
                "\n",
                "**Gini Index (CART Algorithm)**:\n",
                "Gini measures impurity as: Gini = 1 - Œ£(pi¬≤). It tends to favor features \n",
                "that create larger partitions with one dominant class. Gini is less \n",
                "computationally expensive (no logarithms) and often produces similar trees \n",
                "to entropy-based methods. In our dataset, Gini selected feature index \n",
                "{dt_gini.tree_.feature[0]} as root, which corresponds to '{features[dt_gini.tree_.feature[0]]}'.\n",
                "\n",
                "RECOMMENDATION FOR THIS DATASET:\n",
                "\n",
                "Given our small dataset (24 instances) with relatively balanced feature \n",
                "cardinalities (2-3 values each), the bias in Information Gain is minimal. \n",
                "However, for production systems with mixed-cardinality features:\n",
                "\n",
                "1. Use **Gain Ratio** when features have varying numbers of categories\n",
                "2. Use **Gini Index** for computational efficiency with similar results\n",
                "3. Always validate with cross-validation to detect overfitting\n",
                "\n",
                "For our specific case, the current IG-based selection of '{best_feature}' \n",
                "as the root node appears reasonable, as all features have similar cardinality \n",
                "and the choice reflects genuine predictive value rather than cardinality bias.\n",
                "\n",
                "Word Count: ~380 words\n",
                "\"\"\"\n",
                "\n",
                "print(essay)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the generated dataset\n",
                "df.to_csv('data/q7_generated_dataset.csv', index=False)\n",
                "df_modified.to_csv('data/q11_modified_dataset.csv', index=False)\n",
                "\n",
                "print(\"\\n‚úÖ Datasets saved:\")\n",
                "print(\"   - data/q7_generated_dataset.csv (Original)\")\n",
                "print(\"   - data/q11_modified_dataset.csv (Modified for Q11)\")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"SECTION G COMPLETE\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"\"\"\n",
                "Summary:\n",
                "‚Ä¢ Q7:  Generated unique dataset with seed {STUDENT_ID}\n",
                "‚Ä¢ Q8:  Calculated class entropy = {entropy:.4f} bits\n",
                "‚Ä¢ Q9:  Computed IG for all features, best = {best_feature}\n",
                "‚Ä¢ Q10: Constructed decision tree with root = {best_feature}\n",
                "‚Ä¢ Q11: Modified instance, entropy changed by {new_entropy-entropy:+.4f}\n",
                "‚Ä¢ Q12: Essay on IG bias (~380 words)\n",
                "\"\"\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}