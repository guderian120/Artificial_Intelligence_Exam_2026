{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# WMI607: AI and Machine Learning Take-Home Exam\n",
                "## Section C: Feature Engineering & Representation (Question 3)\n",
                "\n",
                "**Objective**: Apply and justify feature engineering techniques with domain knowledge.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "import json\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load preprocessed data\n",
                "import os\n",
                "\n",
                "# Check if preprocessed data exists, otherwise load from Kaggle\n",
                "if os.path.exists('data/preprocessed_data.pkl'):\n",
                "    df = pd.read_pickle('data/preprocessed_data.pkl')\n",
                "    with open('data/metadata.json', 'r') as f:\n",
                "        metadata = json.load(f)\n",
                "    target_col = metadata['target_column']\n",
                "    print(f\"Loaded preprocessed data: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
                "else:\n",
                "    import kagglehub\n",
                "    path = kagglehub.dataset_download(\"ziya07/iiot-edge-computing-dataset\")\n",
                "    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
                "    df = pd.read_csv(os.path.join(path, csv_files[0]))\n",
                "    # Identify target column\n",
                "    potential_targets = [col for col in df.columns if any(t in col.lower() for t in ['target', 'label', 'class', 'anomaly', 'attack'])]\n",
                "    target_col = potential_targets[0] if potential_targets else df.columns[-1]\n",
                "    print(f\"Loaded data from Kaggle: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
                "\n",
                "print(f\"Target column: {target_col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Feature Analysis and Domain Knowledge\n",
                "\n",
                "### Domain Context: IIoT Edge Computing & Infrastructure Monitoring\n",
                "\n",
                "The dataset contains sensor readings from Industrial IoT edge computing devices. Key domain insights:\n",
                "\n",
                "1. **Sensor readings** often have drift and calibration issues\n",
                "2. **Temporal patterns** are crucial for anomaly detection\n",
                "3. **Cross-sensor correlations** reveal system state\n",
                "4. **Statistical features** capture distribution changes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and target\n",
                "X = df.drop(columns=[target_col])\n",
                "y = df[target_col]\n",
                "\n",
                "# Identify numeric and categorical columns\n",
                "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
                "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
                "\n",
                "print(f\"\\nFeature Summary:\")\n",
                "print(f\"  â€¢ Numeric features: {len(numeric_cols)}\")\n",
                "print(f\"  â€¢ Categorical features: {len(categorical_cols)}\")\n",
                "print(f\"  â€¢ Target classes: {y.nunique()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Feature Representation Technique 1: Statistical Feature Engineering\n",
                "\n",
                "**Justification**: IIoT sensor data exhibits statistical patterns that raw values don't capture. Creating statistical features helps models understand data distribution characteristics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Representation 1: Statistical Features\n",
                "print(\"=\" * 60)\n",
                "print(\"FEATURE REPRESENTATION 1: STATISTICAL ENGINEERING\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "X_numeric = X[numeric_cols].copy()\n",
                "\n",
                "# Handle any remaining missing values\n",
                "X_numeric = X_numeric.fillna(X_numeric.median())\n",
                "\n",
                "# Create statistical features\n",
                "stat_features = pd.DataFrame(index=X_numeric.index)\n",
                "\n",
                "# Row-wise statistics (for each sample across all features)\n",
                "stat_features['row_mean'] = X_numeric.mean(axis=1)\n",
                "stat_features['row_std'] = X_numeric.std(axis=1)\n",
                "stat_features['row_min'] = X_numeric.min(axis=1)\n",
                "stat_features['row_max'] = X_numeric.max(axis=1)\n",
                "stat_features['row_range'] = stat_features['row_max'] - stat_features['row_min']\n",
                "stat_features['row_skew'] = X_numeric.skew(axis=1)\n",
                "stat_features['row_kurtosis'] = X_numeric.kurtosis(axis=1)\n",
                "\n",
                "# Percentile-based features\n",
                "stat_features['row_q25'] = X_numeric.quantile(0.25, axis=1)\n",
                "stat_features['row_q75'] = X_numeric.quantile(0.75, axis=1)\n",
                "stat_features['row_iqr'] = stat_features['row_q75'] - stat_features['row_q25']\n",
                "\n",
                "# Combine with original features\n",
                "X_stat = pd.concat([X_numeric, stat_features], axis=1)\n",
                "\n",
                "print(f\"\\nðŸ“Š Statistical Features Created:\")\n",
                "print(f\"   â€¢ Original features: {len(numeric_cols)}\")\n",
                "print(f\"   â€¢ New statistical features: {len(stat_features.columns)}\")\n",
                "print(f\"   â€¢ Total features: {X_stat.shape[1]}\")\n",
                "print(f\"\\n   Features added: {list(stat_features.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate impact of statistical features\n",
                "from sklearn.model_selection import cross_val_score\n",
                "\n",
                "# Baseline model (original features only)\n",
                "scaler = StandardScaler()\n",
                "X_orig_scaled = scaler.fit_transform(X_numeric)\n",
                "\n",
                "rf_baseline = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
                "scores_baseline = cross_val_score(rf_baseline, X_orig_scaled, y, cv=5, scoring='accuracy')\n",
                "\n",
                "# Model with statistical features\n",
                "X_stat_scaled = scaler.fit_transform(X_stat)\n",
                "rf_stat = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
                "scores_stat = cross_val_score(rf_stat, X_stat_scaled, y, cv=5, scoring='accuracy')\n",
                "\n",
                "print(f\"\\nðŸ“ˆ PERFORMANCE COMPARISON:\")\n",
                "print(f\"   Baseline (original features): {scores_baseline.mean():.4f} (+/- {scores_baseline.std()*2:.4f})\")\n",
                "print(f\"   With statistical features:    {scores_stat.mean():.4f} (+/- {scores_stat.std()*2:.4f})\")\n",
                "print(f\"   Improvement: {(scores_stat.mean() - scores_baseline.mean())*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Feature Representation Technique 2: Dimensionality Reduction with PCA\n",
                "\n",
                "**Justification**: High-dimensional IIoT data often contains redundant information. PCA extracts principal components that capture maximum variance while reducing dimensionality for faster training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Representation 2: PCA-based Representation\n",
                "print(\"=\" * 60)\n",
                "print(\"FEATURE REPRESENTATION 2: PCA DIMENSIONALITY REDUCTION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Scale data for PCA\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X_numeric)\n",
                "\n",
                "# Apply PCA\n",
                "pca_full = PCA()\n",
                "pca_full.fit(X_scaled)\n",
                "\n",
                "# Calculate cumulative explained variance\n",
                "cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
                "\n",
                "# Find number of components for 95% variance\n",
                "n_components_95 = np.argmax(cumsum >= 0.95) + 1\n",
                "n_components_90 = np.argmax(cumsum >= 0.90) + 1\n",
                "\n",
                "print(f\"\\nðŸ“Š PCA ANALYSIS:\")\n",
                "print(f\"   â€¢ Original dimensions: {X_scaled.shape[1]}\")\n",
                "print(f\"   â€¢ Components for 90% variance: {n_components_90}\")\n",
                "print(f\"   â€¢ Components for 95% variance: {n_components_95}\")\n",
                "print(f\"   â€¢ Dimensionality reduction: {(1 - n_components_95/X_scaled.shape[1])*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize PCA variance\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Explained variance by component\n",
                "ax1 = axes[0]\n",
                "n_show = min(20, len(pca_full.explained_variance_ratio_))\n",
                "ax1.bar(range(1, n_show+1), pca_full.explained_variance_ratio_[:n_show], alpha=0.7, label='Individual')\n",
                "ax1.plot(range(1, n_show+1), cumsum[:n_show], 'r-o', linewidth=2, label='Cumulative')\n",
                "ax1.axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
                "ax1.set_xlabel('Principal Component', fontsize=12)\n",
                "ax1.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
                "ax1.set_title('PCA Explained Variance Analysis', fontsize=14, fontweight='bold')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 2: 2D PCA projection\n",
                "ax2 = axes[1]\n",
                "pca_2d = PCA(n_components=2)\n",
                "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
                "\n",
                "# Sample for visualization if dataset is large\n",
                "sample_size = min(5000, len(X_pca_2d))\n",
                "indices = np.random.choice(len(X_pca_2d), sample_size, replace=False)\n",
                "\n",
                "scatter = ax2.scatter(X_pca_2d[indices, 0], X_pca_2d[indices, 1], \n",
                "                      c=y.iloc[indices].astype('category').cat.codes, \n",
                "                      cmap='viridis', alpha=0.5, s=10)\n",
                "ax2.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% var)', fontsize=12)\n",
                "ax2.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% var)', fontsize=12)\n",
                "ax2.set_title('2D PCA Projection of Data', fontsize=14, fontweight='bold')\n",
                "plt.colorbar(scatter, ax=ax2, label='Class')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('images/02_pca_analysis.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"\\nâœ… Figure saved: images/02_pca_analysis.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate PCA-transformed features\n",
                "pca_opt = PCA(n_components=n_components_95)\n",
                "X_pca = pca_opt.fit_transform(X_scaled)\n",
                "\n",
                "rf_pca = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
                "scores_pca = cross_val_score(rf_pca, X_pca, y, cv=5, scoring='accuracy')\n",
                "\n",
                "print(f\"\\nðŸ“ˆ PCA PERFORMANCE COMPARISON:\")\n",
                "print(f\"   Baseline (original {X_scaled.shape[1]} features): {scores_baseline.mean():.4f} (+/- {scores_baseline.std()*2:.4f})\")\n",
                "print(f\"   PCA ({n_components_95} components):              {scores_pca.mean():.4f} (+/- {scores_pca.std()*2:.4f})\")\n",
                "print(f\"   Accuracy change: {(scores_pca.mean() - scores_baseline.mean())*100:+.2f}%\")\n",
                "print(f\"   Feature reduction: {(1 - n_components_95/X_scaled.shape[1])*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Feature Selection: Information-Theoretic Approach\n",
                "\n",
                "**Justification**: Select the most informative features using mutual information, which captures non-linear dependencies that correlation-based methods miss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Selection using Mutual Information\n",
                "print(\"=\" * 60)\n",
                "print(\"FEATURE SELECTION: MUTUAL INFORMATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Calculate mutual information scores\n",
                "mi_scores = mutual_info_classif(X_scaled, y, random_state=42)\n",
                "mi_df = pd.DataFrame({'Feature': numeric_cols, 'MI_Score': mi_scores})\n",
                "mi_df = mi_df.sort_values('MI_Score', ascending=False)\n",
                "\n",
                "print(f\"\\nðŸ“Š TOP 15 FEATURES BY MUTUAL INFORMATION:\")\n",
                "print(mi_df.head(15).to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature importance\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Plot 1: Mutual Information scores\n",
                "ax1 = axes[0]\n",
                "top_n = 15\n",
                "top_features = mi_df.head(top_n)\n",
                "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, top_n))\n",
                "bars = ax1.barh(range(top_n), top_features['MI_Score'].values[::-1], color=colors[::-1])\n",
                "ax1.set_yticks(range(top_n))\n",
                "ax1.set_yticklabels(top_features['Feature'].values[::-1])\n",
                "ax1.set_xlabel('Mutual Information Score', fontsize=12)\n",
                "ax1.set_title('Top 15 Features by Mutual Information', fontsize=14, fontweight='bold')\n",
                "ax1.grid(True, alpha=0.3, axis='x')\n",
                "\n",
                "# Plot 2: Feature selection comparison\n",
                "ax2 = axes[1]\n",
                "k_values = [5, 10, 15, 20, 25, 30]\n",
                "k_scores = []\n",
                "for k in k_values:\n",
                "    top_k_features = mi_df.head(k)['Feature'].tolist()\n",
                "    X_selected = X_numeric[top_k_features]\n",
                "    X_sel_scaled = StandardScaler().fit_transform(X_selected)\n",
                "    rf_sel = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
                "    scores = cross_val_score(rf_sel, X_sel_scaled, y, cv=3, scoring='accuracy')\n",
                "    k_scores.append(scores.mean())\n",
                "\n",
                "ax2.plot(k_values, k_scores, 'bo-', linewidth=2, markersize=8)\n",
                "ax2.axhline(y=scores_baseline.mean(), color='r', linestyle='--', label='All features baseline')\n",
                "ax2.set_xlabel('Number of Selected Features', fontsize=12)\n",
                "ax2.set_ylabel('Cross-Validation Accuracy', fontsize=12)\n",
                "ax2.set_title('Accuracy vs Number of Features', fontsize=14, fontweight='bold')\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('images/02_feature_importance.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"\\nâœ… Figure saved: images/02_feature_importance.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary comparison table\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"FEATURE ENGINEERING SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "summary_data = {\n",
                "    'Representation': [\n",
                "        'Original Features',\n",
                "        'Statistical Features',\n",
                "        f'PCA ({n_components_95} components)',\n",
                "        f'Top-{top_n} by MI'\n",
                "    ],\n",
                "    'Num Features': [\n",
                "        len(numeric_cols),\n",
                "        X_stat.shape[1],\n",
                "        n_components_95,\n",
                "        top_n\n",
                "    ],\n",
                "    'CV Accuracy': [\n",
                "        f'{scores_baseline.mean():.4f}',\n",
                "        f'{scores_stat.mean():.4f}',\n",
                "        f'{scores_pca.mean():.4f}',\n",
                "        f'{k_scores[k_values.index(15)]:.4f}' if 15 in k_values else 'N/A'\n",
                "    ],\n",
                "    'Std Dev': [\n",
                "        f'{scores_baseline.std():.4f}',\n",
                "        f'{scores_stat.std():.4f}',\n",
                "        f'{scores_pca.std():.4f}',\n",
                "        'N/A'\n",
                "    ]\n",
                "}\n",
                "\n",
                "summary_df = pd.DataFrame(summary_data)\n",
                "print(\"\\n\")\n",
                "print(summary_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save engineered features for next notebook\n",
                "best_features = mi_df.head(20)['Feature'].tolist()\n",
                "X_engineered = X_stat[best_features + list(stat_features.columns)]\n",
                "\n",
                "# Save to file\n",
                "X_engineered.to_pickle('data/engineered_features.pkl')\n",
                "y.to_pickle('data/target.pkl')\n",
                "\n",
                "# Update metadata\n",
                "with open('data/metadata.json', 'r') as f:\n",
                "    metadata = json.load(f)\n",
                "\n",
                "metadata['selected_features'] = best_features\n",
                "metadata['statistical_features'] = list(stat_features.columns)\n",
                "metadata['pca_components_95'] = n_components_95\n",
                "\n",
                "with open('data/metadata.json', 'w') as f:\n",
                "    json.dump(metadata, f, indent=2)\n",
                "\n",
                "print(\"\\nâœ… Engineered features saved to 'data/engineered_features.pkl'\")\n",
                "print(\"âœ… Target variable saved to 'data/target.pkl'\")\n",
                "print(\"âœ… Metadata updated with feature engineering info\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Summary\n",
                "\n",
                "### Feature Engineering Techniques Applied:\n",
                "\n",
                "| Technique | Description | Domain Justification | Impact |\n",
                "|-----------|-------------|---------------------|--------|\n",
                "| Statistical Features | Row-wise mean, std, skew, kurtosis, IQR | Captures distribution changes in sensor readings | Improved accuracy |\n",
                "| PCA Reduction | Extract principal components for 95% variance | Removes redundant information, speeds up training | Minimal accuracy loss, faster training |\n",
                "| Mutual Information Selection | Select top features by information content | Identifies most predictive features for anomaly detection | Focused model with key features |\n",
                "\n",
                "### Key Insights:\n",
                "1. Statistical features enhance model understanding of data distribution\n",
                "2. PCA achieves significant dimensionality reduction with minimal information loss\n",
                "3. Feature selection identifies the most informative features for the classification task\n",
                "\n",
                "**Next Steps**: Machine Learning Model Design (Notebook 03)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}