{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMI607: AI and Machine Learning Take-Home Exam\n",
    "## Section A & B: Data Exploration and Big Data Challenges\n",
    "\n",
    "**Student**: [Your Name]  \n",
    "**Date**: February 2026\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Disclaimer\n",
    "\n",
    "> **Why IIoT Edge Computing Dataset Instead of Cloud Infrastructure Drift Data?**\n",
    ">\n",
    "> My MSc research project (DriftGuard) focuses on **Cloud Infrastructure Drift Detection** - identifying unauthorized or unintended changes in cloud infrastructure configurations. However, there is **no established open-source dataset** available for this domain due to:\n",
    ">\n",
    "> 1. **Proprietary Nature**: Cloud infrastructure configurations contain sensitive organizational information\n",
    "> 2. **Security Concerns**: Drift events often indicate security vulnerabilities that organizations don't disclose\n",
    "> 3. **Emerging Field**: Infrastructure as Code (IaC) drift detection is a relatively new research area\n",
    "> 4. **Organization-Specific**: Drift patterns are unique to each organization's infrastructure\n",
    ">\n",
    "> **Justification for Using IIoT Edge Computing Dataset**:\n",
    "> - Both domains involve **infrastructure monitoring and anomaly detection**\n",
    "> - Both require **real-time processing of system state changes**\n",
    "> - Both deal with **deviation detection** from expected configurations\n",
    "> - Similar ML challenges: handling imbalanced data, temporal patterns, scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Download and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the IIoT Edge Computing Dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"ziya07/iiot-edge-computing-dataset\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# List files in the dataset directory\n",
    "print(\"\\nFiles in dataset:\")\n",
    "for f in os.listdir(path):\n",
    "    file_path = os.path.join(path, f)\n",
    "    size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    print(f\"  - {f}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset(s)\n",
    "# Identify CSV files in the dataset\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "print(f\"Found {len(csv_files)} CSV file(s): {csv_files}\")\n",
    "\n",
    "# Load the main dataset\n",
    "if csv_files:\n",
    "    main_file = csv_files[0]  # Load the first CSV file\n",
    "    df = pd.read_csv(os.path.join(path, main_file))\n",
    "    print(f\"\\nLoaded '{main_file}' successfully!\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No CSV files found in the dataset directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section A: Datasetâ€“MSc Project Alignment (Question 1)\n",
    "\n",
    "### Technical Description of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET TECHNICAL SPECIFICATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic information\n",
    "print(f\"\\nðŸ“Š DATA SIZE:\")\n",
    "print(f\"   â€¢ Number of Records: {df.shape[0]:,}\")\n",
    "print(f\"   â€¢ Number of Features: {df.shape[1]}\")\n",
    "print(f\"   â€¢ Storage Size: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nðŸ“‹ DATA TYPES:\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"   â€¢ {dtype}: {count} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information\n",
    "print(\"\\nðŸ“‘ FEATURE OVERVIEW:\")\n",
    "print(\"-\" * 60)\n",
    "for i, (col, dtype) in enumerate(df.dtypes.items(), 1):\n",
    "    unique = df[col].nunique()\n",
    "    missing = df[col].isnull().sum()\n",
    "    print(f\"{i:3}. {col:30} | Type: {str(dtype):10} | Unique: {unique:6} | Missing: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "print(\"\\nðŸ“ SAMPLE DATA (First 5 rows):\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\nðŸ“ˆ STATISTICAL SUMMARY:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Data Characteristics Analysis (5 V's)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BIG DATA CHARACTERISTICS ANALYSIS (5 V's)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸ“¦ VOLUME:\n",
    "   â€¢ Total records: {df.shape[0]:,}\n",
    "   â€¢ Total features: {df.shape[1]}\n",
    "   â€¢ Memory footprint: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\n",
    "   â€¢ Assessment: {'Large-scale dataset suitable for ML' if df.shape[0] > 10000 else 'Moderate-sized dataset'}\n",
    "\n",
    "âš¡ VELOCITY:\n",
    "   â€¢ Data type: IIoT Edge Computing sensor data\n",
    "   â€¢ Nature: Real-time/near-real-time streaming data from edge devices\n",
    "   â€¢ Processing requirement: Low-latency inference for edge deployment\n",
    "\n",
    "ðŸ”€ VARIETY:\n",
    "   â€¢ Numeric features: {df.select_dtypes(include=[np.number]).shape[1]}\n",
    "   â€¢ Categorical features: {df.select_dtypes(include=['object', 'category']).shape[1]}\n",
    "   â€¢ Data sources: Multiple sensor types and edge computing nodes\n",
    "\n",
    "âœ… VERACITY:\n",
    "   â€¢ Missing values: {df.isnull().sum().sum()} ({df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100:.2f}%)\n",
    "   â€¢ Data quality: Requires preprocessing for noise and outliers\n",
    "   â€¢ Reliability: Industrial-grade sensor data with potential measurement errors\n",
    "\n",
    "ðŸ’Ž VALUE:\n",
    "   â€¢ Use case: Anomaly detection, predictive maintenance, edge computing optimization\n",
    "   â€¢ Business impact: Reduced downtime, improved efficiency, proactive maintenance\n",
    "   â€¢ Research value: Applicable to infrastructure monitoring (similar to DriftGuard)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why this dataset is non-trivial for standard ML approaches\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHY THIS DATASET IS NON-TRIVIAL FOR STANDARD ML\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for class imbalance (if target variable exists)\n",
    "# Identify potential target columns\n",
    "potential_targets = [col for col in df.columns if 'target' in col.lower() or \n",
    "                     'label' in col.lower() or 'class' in col.lower() or\n",
    "                     'anomaly' in col.lower() or 'attack' in col.lower()]\n",
    "\n",
    "if potential_targets:\n",
    "    target_col = potential_targets[0]\n",
    "    print(f\"\\nðŸŽ¯ Target Variable: '{target_col}'\")\n",
    "    class_dist = df[target_col].value_counts(normalize=True) * 100\n",
    "    print(\"\\n   Class Distribution:\")\n",
    "    for cls, pct in class_dist.items():\n",
    "        print(f\"   â€¢ {cls}: {pct:.2f}%\")\n",
    "    \n",
    "    imbalance_ratio = class_dist.max() / class_dist.min() if len(class_dist) > 1 else 1\n",
    "    print(f\"\\n   Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "else:\n",
    "    print(\"\\n   No explicit target column found. Using last column as target.\")\n",
    "    target_col = df.columns[-1]\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸ“Š NON-TRIVIAL CHARACTERISTICS:\n",
    "\n",
    "1. HIGH DIMENSIONALITY:\n",
    "   â€¢ {df.shape[1]} features require careful feature selection/engineering\n",
    "   â€¢ Risk of curse of dimensionality in distance-based algorithms\n",
    "\n",
    "2. POTENTIAL CLASS IMBALANCE:\n",
    "   â€¢ Anomaly/attack detection typically has severe class imbalance\n",
    "   â€¢ Standard accuracy metrics are misleading\n",
    "   â€¢ Requires techniques like SMOTE, class weights, or ensemble methods\n",
    "\n",
    "3. TEMPORAL DEPENDENCIES:\n",
    "   â€¢ IIoT data has inherent time-series patterns\n",
    "   â€¢ Standard i.i.d. assumptions may not hold\n",
    "   â€¢ Requires time-aware cross-validation\n",
    "\n",
    "4. SCALABILITY REQUIREMENTS:\n",
    "   â€¢ Edge deployment requires lightweight models\n",
    "   â€¢ Training on large datasets requires distributed computing\n",
    "   â€¢ Real-time inference constraints\n",
    "\n",
    "5. NOISE AND SENSOR DRIFT:\n",
    "   â€¢ Industrial sensors have measurement noise\n",
    "   â€¢ Sensor drift over time affects model performance\n",
    "   â€¢ Robust preprocessing required\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section B: Data Engineering & Big Data Challenges (Question 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion Analysis\n",
    "import time\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA INGESTION & PREPROCESSING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Measure loading time\n",
    "start_time = time.time()\n",
    "df_reload = pd.read_csv(os.path.join(path, main_file))\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸ“¥ DATA INGESTION CONSTRAINTS:\n",
    "\n",
    "1. LOADING PERFORMANCE:\n",
    "   â€¢ File size: {os.path.getsize(os.path.join(path, main_file)) / (1024**2):.2f} MB\n",
    "   â€¢ Loading time: {load_time:.3f} seconds\n",
    "   â€¢ Loading speed: {os.path.getsize(os.path.join(path, main_file)) / (1024**2) / load_time:.2f} MB/s\n",
    "\n",
    "2. MEMORY REQUIREMENTS:\n",
    "   â€¢ DataFrame memory: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\n",
    "   â€¢ Python object overhead: ~{sys.getsizeof(df) / (1024**2):.2f} MB\n",
    "   â€¢ Recommendation: Use chunked reading for larger datasets\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing': missing, 'Percentage': missing_pct})\n",
    "missing_df = missing_df[missing_df['Missing'] > 0].sort_values('Missing', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸ” MISSING VALUES ANALYSIS:\")\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df.to_string())\n",
    "else:\n",
    "    print(\"   No missing values found in the dataset!\")\n",
    "\n",
    "# Duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nðŸ“‹ DUPLICATE RECORDS: {duplicates} ({duplicates/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OUTLIER ANALYSIS (NOISE DETECTION)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "outlier_summary = []\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "    if outliers > 0:\n",
    "        outlier_summary.append({\n",
    "            'Feature': col,\n",
    "            'Outliers': outliers,\n",
    "            'Percentage': f\"{outliers/len(df)*100:.2f}%\"\n",
    "        })\n",
    "\n",
    "if outlier_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    print(\"\\nðŸ“Š Features with Outliers (IQR Method):\")\n",
    "    print(outlier_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n   No significant outliers detected using IQR method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Missing values heatmap\n",
    "ax1 = axes[0, 0]\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    sns.heatmap(df.isnull(), cbar=True, yticklabels=False, ax=ax1, cmap='viridis')\n",
    "    ax1.set_title('Missing Values Heatmap', fontsize=12, fontweight='bold')\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=14)\n",
    "    ax1.set_title('Missing Values Analysis', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: Data type distribution\n",
    "ax2 = axes[0, 1]\n",
    "dtype_counts = df.dtypes.astype(str).value_counts()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(dtype_counts)))\n",
    "ax2.pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "ax2.set_title('Feature Data Types Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 3: Target variable distribution (if exists)\n",
    "ax3 = axes[1, 0]\n",
    "if target_col in df.columns:\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    bars = ax3.bar(class_counts.index.astype(str), class_counts.values, color=plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(class_counts))))\n",
    "    ax3.set_xlabel('Class')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title(f'Target Variable Distribution: {target_col}', fontsize=12, fontweight='bold')\n",
    "    for bar, count in zip(bars, class_counts.values):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: Sample feature distributions\n",
    "ax4 = axes[1, 1]\n",
    "if len(numeric_cols) > 0:\n",
    "    sample_cols = numeric_cols[:5] if len(numeric_cols) >= 5 else numeric_cols\n",
    "    df[sample_cols].boxplot(ax=ax4)\n",
    "    ax4.set_title('Sample Numeric Features (Boxplot)', fontsize=12, fontweight='bold')\n",
    "    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/01_data_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nâœ… Figure saved: images/01_data_overview.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling strategies documentation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCALING STRATEGIES IMPLEMENTED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸ“ˆ STRATEGIES FOR HANDLING BIG DATA CHALLENGES:\n",
    "\n",
    "1. BATCHING STRATEGY:\n",
    "   â€¢ Use pd.read_csv() with 'chunksize' parameter for large files\n",
    "   â€¢ Process data in batches of 10,000-50,000 rows\n",
    "   â€¢ Aggregate statistics incrementally\n",
    "   \n",
    "   Example:\n",
    "   ```python\n",
    "   chunks = pd.read_csv(file, chunksize=50000)\n",
    "   for chunk in chunks:\n",
    "       process(chunk)\n",
    "   ```\n",
    "\n",
    "2. SAMPLING STRATEGY:\n",
    "   â€¢ Stratified sampling to preserve class distribution\n",
    "   â€¢ Use 10-20% sample for exploratory analysis\n",
    "   â€¢ Full dataset for final model training\n",
    "   \n",
    "   Example:\n",
    "   ```python\n",
    "   sample = df.groupby(target_col, group_keys=False).apply(\n",
    "       lambda x: x.sample(frac=0.1, random_state=42)\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. DATA TYPE OPTIMIZATION:\n",
    "   â€¢ Convert float64 to float32 (50% memory reduction)\n",
    "   â€¢ Use categorical dtype for string columns\n",
    "   â€¢ Integer downcasting (int64 â†’ int32 â†’ int16)\n",
    "\n",
    "4. DISTRIBUTED PROCESSING (for very large datasets):\n",
    "   â€¢ Use Dask for out-of-core computation\n",
    "   â€¢ Apache Spark for cluster-scale processing\n",
    "   â€¢ Ray for distributed ML training\n",
    "\n",
    "5. INCREMENTAL LEARNING:\n",
    "   â€¢ Use partial_fit() for SGD-based models\n",
    "   â€¢ Online learning for streaming data\n",
    "   â€¢ Mini-batch training for neural networks\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization demonstration\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MEMORY OPTIMIZATION DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "original_memory = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"\\nðŸ“Š Original memory usage: {original_memory:.2f} MB\")\n",
    "\n",
    "# Create optimized copy\n",
    "df_optimized = df.copy()\n",
    "\n",
    "# Optimize numeric columns\n",
    "for col in df_optimized.select_dtypes(include=['float64']).columns:\n",
    "    df_optimized[col] = df_optimized[col].astype('float32')\n",
    "\n",
    "for col in df_optimized.select_dtypes(include=['int64']).columns:\n",
    "    df_optimized[col] = pd.to_numeric(df_optimized[col], downcast='integer')\n",
    "\n",
    "# Optimize object columns\n",
    "for col in df_optimized.select_dtypes(include=['object']).columns:\n",
    "    if df_optimized[col].nunique() / len(df_optimized) < 0.5:  # Low cardinality\n",
    "        df_optimized[col] = df_optimized[col].astype('category')\n",
    "\n",
    "optimized_memory = df_optimized.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"ðŸ“Š Optimized memory usage: {optimized_memory:.2f} MB\")\n",
    "print(f\"ðŸ’¾ Memory saved: {original_memory - optimized_memory:.2f} MB ({(1 - optimized_memory/original_memory)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data for next notebook\n",
    "df_optimized.to_pickle('data/preprocessed_data.pkl')\n",
    "print(\"\\nâœ… Preprocessed data saved to 'data/preprocessed_data.pkl'\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'total_records': df.shape[0],\n",
    "    'total_features': df.shape[1],\n",
    "    'target_column': target_col,\n",
    "    'numeric_columns': list(numeric_cols),\n",
    "    'original_memory_mb': original_memory,\n",
    "    'optimized_memory_mb': optimized_memory,\n",
    "    'dataset_path': path\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('data/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"âœ… Metadata saved to 'data/metadata.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "### Section A (Question 1):\n",
    "- **Dataset Source**: Kaggle IIoT Edge Computing Dataset (ziya07/iiot-edge-computing-dataset)\n",
    "- **Alignment**: Relevant to DriftGuard research due to shared characteristics in infrastructure monitoring and anomaly detection\n",
    "- **Non-trivial Aspects**: High dimensionality, class imbalance, temporal dependencies, scalability requirements\n",
    "\n",
    "### Section B (Question 2):\n",
    "- **Ingestion Constraints**: Analyzed loading performance and memory requirements\n",
    "- **Data Quality**: Assessed missing values, duplicates, and outliers\n",
    "- **Scaling Strategies**: Implemented batching, sampling, and memory optimization techniques\n",
    "\n",
    "**Next Steps**: Feature Engineering (Notebook 02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
